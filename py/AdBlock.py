import requests
from datetime import datetime, timedelta, timezone
import os

# è§„åˆ™æºåœ°å€æ˜ å°„ï¼ˆåœ°å€ï¼šæ ‡ç­¾ï¼‰
rule_sources = {
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/fq2.txt": "fq2.txt",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/fqnovel-fxxk_ads": "fqnovel-fxxk_ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/qimao-ads": "qimao-ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/b_ads": "b_ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/aiqiyi": "aiqiyi",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/genery_rules": "genery_rules",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/xmlyjsb": "xmlyjsb",
    "https://ghfast.top/raw.githubusercontent.com/Star7-Files-Hub/Files/refs/heads/main/Ad/Manual_input.txt": "Manual_input.txt",
    "https://ghfast.top/raw.githubusercontent.com/8680/GOODBYEADS/master/data/rules/dns.txt": "dns.txt",
    "https://raw.githubusercontent.com/banbendalao/ADgk/master/ADgk.txt": "ADgk.txt",
    "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/Filters/AWAvenue-Ads-Rule-hosts.txt": "AWAvenue-Ads-Rule-hosts.txt",
    "http://rssv.cn/adguard/api.php?type=black": "QY.txt",
    "https://github.com/miaoermua/AdguardFilter/blob/main/rule.txt": "rules.txt",
    "https://raw.githubusercontent.com/miaoermua/AdguardFilter/main/bad_apple.txt": "bad_apple.txt"
}

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("Ad", exist_ok=True)

unique_rules = set()
duplicate_rules = set()
combined_rules = []
log_lines = []

# å½“å‰åŒ—äº¬æ—¶é—´
beijing_time = datetime.utcnow() + timedelta(hours=8)
version_str = beijing_time.strftime("%Y-%m-%d %H:%M:%Sï¼ˆåŒ—äº¬æ—¶é—´ï¼‰")

total_unique = 0

# åˆå¹¶è§„åˆ™å¹¶å»é‡
for url, label in rule_sources.items():
    try:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½è§„åˆ™ï¼š{label}")
        resp = requests.get(url)
        resp.raise_for_status()
        lines = resp.text.strip().splitlines()

        rules_added = 0
        duplicates = 0
        combined_rules.append(f"! Source: {url} ({label})")

        for line in lines:
            stripped = line.strip()
            if not stripped or stripped.startswith("!"):
                continue
            if stripped in unique_rules:
                duplicate_rules.add(stripped)
                duplicates += 1
                continue
            unique_rules.add(stripped)
            combined_rules.append(stripped)
            rules_added += 1

        combined_rules.append("")  # ç©ºè¡Œåˆ†éš”
        total_unique += rules_added
        log_lines.append(f"[{label}] æ·»åŠ è§„åˆ™ï¼š{rules_added} æ¡ï¼Œé‡å¤è§„åˆ™ï¼š{duplicates} æ¡")

    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥ {url}ï¼š{e}")
        log_lines.append(f"[{label}] ä¸‹è½½å¤±è´¥ï¼š{e}")

# æ„å»ºè§„åˆ™æ–‡ä»¶å¤´éƒ¨ï¼ˆå…ƒä¿¡æ¯ï¼‰
header = [
    "! Title: 7Star's_Ad_Rules",
    "! Homepage: https://github.com/Star7-Files-Hub/Files/Ad",
    "! Expires: 12 Hours",
    f"! Version: {version_str}",
    "! Description: é€‚ç”¨äºAdGuardçš„å»å¹¿å‘Šè§„åˆ™ï¼Œåˆå¹¶ä¼˜è´¨ä¸Šæ¸¸è§„åˆ™å¹¶å»é‡æ•´ç†æ’åˆ—",
    f"! Total count: {total_unique}",
    ""
]

# å†™å…¥åˆå¹¶è§„åˆ™æ–‡ä»¶
with open("Ad/AdBlock.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(header + combined_rules))

# å†™å…¥æ—¥å¿—æ–‡ä»¶
log_lines.append("")
log_lines.append(f"âš ï¸ å»é‡è§„åˆ™æ€»æ•°ï¼š{len(duplicate_rules)} æ¡")
log_lines.append("")
log_lines.append("ğŸ§¾ è¢«å»é‡çš„è§„åˆ™ï¼ˆå±•ç¤ºå‰ 50 æ¡ï¼‰ï¼š")
for i, rule in enumerate(sorted(duplicate_rules)):
    if i >= 50:
        log_lines.append(f"...ï¼ˆå…± {len(duplicate_rules)} æ¡ï¼Œå·²çœç•¥ï¼‰")
        break
    log_lines.append(rule)

with open("log/merge_log.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(log_lines))

print("âœ… åˆå¹¶å®Œæˆï¼Œç”Ÿæˆæ–‡ä»¶ï¼šAd/AdBlock.txt å’Œ log/merge_log.txt")
