import requests
from datetime import datetime
import os

# è§„åˆ™æºåœ°å€æ˜ å°„ï¼ˆåœ°å€ï¼šæ ‡ç­¾ï¼‰
rule_sources = {
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/fq2.txt": "fq2.txt",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/fqnovel-fxxk_ads": "fqnovel-fxxk_ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/qimao-ads": "qimao-ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/b_ads": "b_ads",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/aiqiyi": "aiqiyi",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/genery_rules": "genery_rules",
    "https://gcore.jsdelivr.net/gh/changzhaoCZ/fqnovel-adrules@refs/heads/main/xmlyjsb": "xmlyjsb",
    "https://ghfast.top/raw.githubusercontent.com/Star7-Files-Hub/Files/refs/heads/main/Ad/Manual_input.txt": "Manual_input.txt",
    "https://ghfast.top/raw.githubusercontent.com/8680/GOODBYEADS/master/data/rules/dns.txt": "dns.txt"
}

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("Ad", exist_ok=True)

unique_rules = set()
duplicate_rules = set()
combined_rules = []
log_lines = []

beijing_tz = timezone(timedelta(hours=8))
timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S UTC+8")
header = [
    "! Auto-generated AdGuard rules",
    f"! Generated at: {timestamp}",
    "! Total rules: <will-be-replaced>",
    ""
]

total_unique = 0

for url, label in rule_sources.items():
    try:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½è§„åˆ™ï¼š{label}")
        resp = requests.get(url)
        resp.raise_for_status()
        lines = resp.text.strip().splitlines()

        rules_added = 0
        duplicates = 0
        combined_rules.append(f"! Source: {url} ({label})")

        for line in lines:
            stripped = line.strip()
            if not stripped or stripped.startswith("!"):
                continue
            if stripped in unique_rules:
                duplicate_rules.add(stripped)
                duplicates += 1
                continue
            unique_rules.add(stripped)
            combined_rules.append(stripped)
            rules_added += 1

        combined_rules.append("")  # ç©ºè¡Œåˆ†éš”

        total_unique += rules_added
        log_lines.append(f"[{label}] æ·»åŠ è§„åˆ™ï¼š{rules_added} æ¡ï¼Œé‡å¤è§„åˆ™ï¼š{duplicates} æ¡")

    except Exception as e:
        print(f"âŒ è·å–å¤±è´¥ {url}ï¼š{e}")
        log_lines.append(f"[{label}] ä¸‹è½½å¤±è´¥ï¼š{e}")

# å†™å…¥åˆå¹¶è§„åˆ™æ–‡ä»¶
header[2] = f"! Total rules: {total_unique}"

with open("Ad/AdBlock.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(header + combined_rules))

# å†™å…¥æ—¥å¿—
log_lines.append("")
log_lines.append(f"âš ï¸ å»é‡è§„åˆ™æ€»æ•°ï¼š{len(duplicate_rules)} æ¡")
log_lines.append("")
log_lines.append("ğŸ§¾ è¢«å»é‡çš„è§„åˆ™ï¼ˆéƒ¨åˆ†å±•ç¤ºï¼‰ï¼š")
for i, rule in enumerate(sorted(duplicate_rules)):
    if i >= 50:
        log_lines.append(f"...ï¼ˆå…± {len(duplicate_rules)} æ¡ï¼Œä»…å±•ç¤ºå‰ 50 æ¡ï¼‰")
        break
    log_lines.append(rule)

with open("Ad/merge_log.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(log_lines))

print("âœ… åˆå¹¶å®Œæˆï¼Œç”Ÿæˆï¼šAd/AdBlock.txt å’Œ Ad/merge_log.txt")
